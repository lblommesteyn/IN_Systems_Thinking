{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Model Evaluation Notebook\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.models import SystemsThinkingClassifier, SubdimensionClassifier\n",
    "from src.preprocessing import DocumentParser, TextCleaner, ContentFilter\n",
    "from src.rag import SemanticRetriever, ContextProcessor\n",
    "\n",
    "# Initialize models and components\n",
    "classifier = SystemsThinkingClassifier(\"models/systems_thinking_classifier\")\n",
    "subdim_classifier = SubdimensionClassifier(\"models/subdimension_classifier\")\n",
    "\n",
    "# Load test data\n",
    "def load_test_data():\n",
    "    \"\"\"Load annotated test data\"\"\"\n",
    "    return pd.read_csv('../data/test_data.csv')\n",
    "\n",
    "test_data = load_test_data()\n",
    "\n",
    "# 1. High-Level Classification Performance\n",
    "print(\"High-Level Classification Performance\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "y_true = test_data['is_systems_thinking']\n",
    "y_pred = []\n",
    "confidences = []\n",
    "\n",
    "for text in test_data['text']:\n",
    "    prediction, confidence = classifier.predict(text)\n",
    "    y_pred.append(prediction)\n",
    "    confidences.append(confidence)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix - High Level Classification')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n",
    "# 2. Confidence Distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(confidences, bins=30)\n",
    "plt.title('Distribution of Prediction Confidences')\n",
    "plt.xlabel('Confidence')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# 3. Subdimension Performance\n",
    "print(\"\\nSubdimension Performance\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Filter for systems thinking examples\n",
    "systems_thinking_data = test_data[test_data['is_systems_thinking']]\n",
    "\n",
    "# Get subdimension predictions\n",
    "subdim_metrics = {\n",
    "    'purpose': {'correct': 0, 'total': 0},\n",
    "    'tensions': {'correct': 0, 'total': 0},\n",
    "    'macro_issue_why': {'correct': 0, 'total': 0},\n",
    "    'macro_issue_how': {'correct': 0, 'total': 0},\n",
    "    'micro_issue_why': {'correct': 0, 'total': 0},\n",
    "    'micro_issue_how': {'correct': 0, 'total': 0},\n",
    "    'collaboration': {'correct': 0, 'total': 0},\n",
    "    'agency': {'correct': 0, 'total': 0}\n",
    "}\n",
    "\n",
    "for _, row in systems_thinking_data.iterrows():\n",
    "    predictions = subdim_classifier.predict(row['text'])\n",
    "    for dim in subdim_metrics:\n",
    "        if predictions[dim] > 0.5 and row[dim] == 1:\n",
    "            subdim_metrics[dim]['correct'] += 1\n",
    "        if row[dim] == 1:\n",
    "            subdim_metrics[dim]['total'] += 1\n",
    "\n",
    "# Calculate accuracy per dimension\n",
    "accuracies = {\n",
    "    dim: metrics['correct'] / metrics['total'] if metrics['total'] > 0 else 0\n",
    "    for dim, metrics in subdim_metrics.items()\n",
    "}\n",
    "\n",
    "# Plot subdimension accuracies\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=list(accuracies.keys()), y=list(accuracies.values()))\n",
    "plt.title('Accuracy by Subdimension')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. Misclassification Analysis\n",
    "print(\"\\nMisclassification Analysis\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "misclassified = test_data[y_true != y_pred].copy()\n",
    "misclassified['confidence'] = [confidences[i] for i in range(len(y_pred)) if y_true[i] != y_pred[i]]\n",
    "\n",
    "# Sort by confidence\n",
    "misclassified_high_conf = misclassified.sort_values('confidence', ascending=False).head(5)\n",
    "\n",
    "print(\"\\nTop 5 High Confidence Misclassifications:\")\n",
    "for _, row in misclassified_high_conf.iterrows():\n",
    "    print(f\"\\nTrue Label: {row['is_systems_thinking']}\")\n",
    "    print(f\"Confidence: {row['confidence']:.3f}\")\n",
    "    print(f\"Text: {row['text'][:200]}...\")\n",
    "\n",
    "# 5. RAG Impact Analysis\n",
    "print(\"\\nRAG Impact Analysis\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Initialize RAG components\n",
    "retriever = SemanticRetriever(vector_store=None, embeddings_generator=None)\n",
    "context_processor = ContextProcessor()\n",
    "\n",
    "# Compare performance with and without RAG\n",
    "rag_comparison = pd.DataFrame(columns=['Text', 'Without_RAG', 'With_RAG'])\n",
    "\n",
    "for _, row in test_data.head(10).iterrows():\n",
    "    # Without RAG\n",
    "    pred_no_rag, conf_no_rag = classifier.predict(row['text'])\n",
    "    \n",
    "    # With RAG\n",
    "    contexts = retriever.retrieve(row['text'])\n",
    "    processed_context = context_processor.assemble_context(contexts)\n",
    "    pred_with_rag, conf_with_rag = classifier.predict(row['text'], context=processed_context)\n",
    "    \n",
    "    rag_comparison = rag_comparison.append({\n",
    "        'Text': row['text'][:100],\n",
    "        'Without_RAG': conf_no_rag,\n",
    "        'With_RAG': conf_with_rag\n",
    "    }, ignore_index=True)\n",
    "\n",
    "# Plot RAG comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "rag_comparison[['Without_RAG', 'With_RAG']].plot(kind='bar')\n",
    "plt.title('Impact of RAG on Prediction Confidence')\n",
    "plt.xlabel('Sample')\n",
    "plt.ylabel('Confidence')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 6. Error Analysis by Text Length\n",
    "test_data['text_length'] = test_data['text'].str.len()\n",
    "error_by_length = pd.DataFrame({\n",
    "    'text_length': test_data['text_length'],\n",
    "    'is_error': y_true != y_pred\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='is_error', y='text_length', data=error_by_length)\n",
    "plt.title('Text Length Distribution for Correct vs Incorrect Predictions')\n",
    "plt.xlabel('Is Error')\n",
    "plt.ylabel('Text Length')\n",
    "plt.show()\n",
    "\n",
    "# 7. Summary Statistics\n",
    "print(\"\\nSummary Statistics\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Overall Accuracy: {(y_true == y_pred).mean():.3f}\")\n",
    "print(f\"Average Confidence: {np.mean(confidences):.3f}\")\n",
    "print(\"\\nSubdimension Performance:\")\n",
    "for dim, acc in accuracies.items():\n",
    "    print(f\"{dim}: {acc:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
